"""
FireGlobe Agent Metrics Generation Agent

This agent generates comprehensive performance metrics for FireGlobe Agent testing
using data from the Knowledge Graph, following the metrics specification in metrics.md
"""

from datetime import datetime, timezone
from typing import Any, Dict, List, Optional
import json
import os
from dotenv import load_dotenv
from uagents import Context, Model, Agent
from hyperon import MeTTa

# Import metrics module components
from metrics.metricsrag import MetricsRAG
from metrics.knowledge import initialize_knowledge_graph
from metrics.utils import (
    LLM, 
    extract_transaction_hashes, 
    count_tool_calls, 
    analyze_action_types,
    extract_gas_data,
    calculate_success_rate,
    analyze_response_times,
    clean_json_response
)

# Load environment variables
load_dotenv()

# Set API keys
ASI_ONE_API_KEY = os.environ.get("ASI_ONE_API_KEY")
AGENTVERSE_API_KEY = os.environ.get("AGENTVERSE_API_KEY")
BACKEND_URL = os.environ.get("BACKEND_URL", "https://backend-739298578243.us-central1.run.app")

if not ASI_ONE_API_KEY:
    raise ValueError("Please set ASI_ONE_API_KEY environment variable")

# Initialize agent
agent = Agent(
    name="fireglobe_agent_metrics_generator",
    port=8080,
    seed="fireglobe agent metrics generator seed phrase",
    mailbox=f"{AGENTVERSE_API_KEY}" if AGENTVERSE_API_KEY else None,
    endpoint=["http://localhost:8080/submit"]
)

# REST API Models
class MetricsGenerationRequest(Model):
    """Request to generate metrics for a conversation"""
    conversation_id: Optional[str] = None  # If None, use last conversation
    
class MetricsGenerationResponse(Model):
    """Response with generated metrics"""
    success: bool
    conversation_id: str
    metrics: Dict[str, Any]
    timestamp: str
    agent_address: str

class LastMetricsResponse(Model):
    """Response with last generated metrics"""
    success: bool
    conversation_id: Optional[str]
    metrics: Optional[Dict[str, Any]]
    timestamp: str
    agent_address: str


def calculate_capability_metrics(conversation_data: Dict[str, Any]) -> Dict[str, Any]:
    """Calculate capability metrics from conversation data."""
    messages = conversation_data.get('messages', [])
    transactions = conversation_data.get('transactions', [])
    
    # Action Success Rate
    success_rate = calculate_success_rate(messages, transactions)
    
    # Action Type Coverage
    action_types = analyze_action_types(messages, transactions)
    
    # Contract Interaction Accuracy
    contract_interactions = sum(1 for action in action_types if action in ['deploy', 'approval', 'mint', 'wrap'])
    total_contract_calls = contract_interactions if contract_interactions > 0 else 1
    contract_accuracy = (contract_interactions / total_contract_calls) * 100
    
    # State Verification Accuracy (balance checks)
    balance_checks = sum(1 for msg in messages if 'balance' in msg.get('content', '').lower() and msg.get('role') == 'agent')
    state_accuracy = 100.0 if balance_checks > 0 else 0.0
    
    # Adaptive Error Recovery
    errors = sum(1 for msg in messages if any(err in msg.get('content', '').lower() for err in ['error', 'failed', 'unable']))
    recoveries = sum(1 for msg in messages if any(rec in msg.get('content', '').lower() for rec in ['retry', 'alternative', 'instead', 'however']))
    recovery_rate = (recoveries / errors * 100) if errors > 0 else 100.0
    
    # Network Handling Score
    network_mentions = sum(1 for msg in messages if any(net in msg.get('content', '').lower() for net in ['mainnet', 'testnet', 'sepolia', 'network']))
    network_score = min(100, network_mentions * 30 + 70)
    
    return {
        "action_success_rate": round(success_rate, 2),
        "action_type_coverage": action_types,
        "contract_interaction_accuracy": round(contract_accuracy, 2),
        "state_verification_accuracy": round(state_accuracy, 2),
        "adaptive_error_recovery": round(recovery_rate, 2),
        "network_handling_score": round(network_score, 2)
    }


def calculate_efficiency_metrics(conversation_data: Dict[str, Any]) -> Dict[str, Any]:
    """Calculate efficiency metrics from conversation data."""
    messages = conversation_data.get('messages', [])
    transactions = conversation_data.get('transactions', [])
    
    # Response Latency
    response_times = analyze_response_times(messages)
    avg_latency = sum(response_times) / len(response_times) if response_times else 0
    
    # Gas metrics
    gas_values = extract_gas_data(transactions)
    avg_gas = sum(gas_values) / len(gas_values) if gas_values else 0
    
    # Gas efficiency (assuming baseline of 21000 for simple transfer)
    baseline_gas = 21000
    gas_efficiency = (baseline_gas / avg_gas * 100) if avg_gas > 0 else 100.0
    gas_efficiency = min(100, gas_efficiency)  # Cap at 100%
    
    # Cost per action
    avg_cost = (avg_gas * 1e-9) if avg_gas > 0 else 0  # Simplified ETH cost
    
    # Transaction consistency (variance in gas)
    consistency = 100 - (max(gas_values) - min(gas_values)) / avg_gas * 100 if len(gas_values) > 1 and avg_gas > 0 else 100
    consistency = max(0, consistency)
    
    # Failure rate
    failure_rate = 100 - calculate_success_rate(messages, transactions)
    
    return {
        "avg_execution_latency_ms": round(avg_latency, 2),
        "avg_gas_used": round(avg_gas, 2),
        "gas_efficiency_percent": round(gas_efficiency, 2),
        "cost_per_successful_action_eth": round(avg_cost, 6),
        "transaction_consistency_percent": round(consistency, 2),
        "failure_rate_percent": round(failure_rate, 2)
    }


def calculate_reliability_metrics(conversation_data: Dict[str, Any]) -> Dict[str, Any]:
    """Calculate reliability and robustness metrics."""
    messages = conversation_data.get('messages', [])
    transactions = conversation_data.get('transactions', [])
    
    # Recovery Rate
    failures = sum(1 for msg in messages if any(fail in msg.get('content', '').lower() for fail in ['error', 'failed', 'unable']))
    retries = sum(1 for msg in messages if 'retry' in msg.get('content', '').lower())
    recovery_rate = (retries / failures * 100) if failures > 0 else 100.0
    
    # Tool Reliability
    tool_calls = count_tool_calls(messages)
    successful_tools = len([tx for tx in transactions if tx.get('success', True)])
    tool_reliability = (successful_tools / tool_calls * 100) if tool_calls > 0 else 100.0
    
    # Execution Determinism (based on consistency)
    determinism = 88.0  # Default reasonable value
    
    # Network Adaptability
    network_errors = sum(1 for msg in messages if 'not supported' in msg.get('content', '').lower() or 'only available' in msg.get('content', '').lower())
    adaptability = max(70, 100 - network_errors * 10)
    
    return {
        "recovery_rate_percent": round(recovery_rate, 2),
        "tool_reliability_percent": round(tool_reliability, 2),
        "execution_determinism_percent": round(determinism, 2),
        "network_adaptability_score": round(adaptability, 2)
    }


def calculate_interaction_metrics(conversation_data: Dict[str, Any]) -> Dict[str, Any]:
    """Calculate interaction and behavioral metrics."""
    messages = conversation_data.get('messages', [])
    transactions = conversation_data.get('transactions', [])
    personality_name = conversation_data.get('personality_name', '')
    
    # Response Latency
    response_times = analyze_response_times(messages)
    avg_response_latency = sum(response_times) / len(response_times) if response_times else 1100
    
    # Instruction Compliance
    user_messages = [msg for msg in messages if msg.get('role') == 'user']
    agent_responses = [msg for msg in messages if msg.get('role') == 'agent']
    compliance = min(100, (len(agent_responses) / len(user_messages) * 100)) if user_messages else 100
    
    # Transparency Score (% of responses with tx hash or specific details)
    transparent_responses = sum(1 for msg in agent_responses if any(
        indicator in msg.get('content', '').lower() 
        for indicator in ['0x', 'transaction', 'balance:', 'hash', 'eth', 'successfully']
    ))
    transparency = (transparent_responses / len(agent_responses) * 100) if agent_responses else 100
    
    # Personality Adherence
    personality_indicators = sum(1 for msg in agent_responses if len(msg.get('content', '')) > 20)
    adherence = min(100, (personality_indicators / len(agent_responses) * 100)) if agent_responses else 90
    
    # Proactive Initiative Count
    proactive_count = sum(1 for msg in agent_responses if any(
        proactive in msg.get('content', '').lower()
        for proactive in ['would you like', 'i can also', 'suggest', 'recommend', 'you might']
    ))
    
    # Conversation Stability
    stability = 97.0  # Default high value
    
    return {
        "response_latency_ms": round(avg_response_latency, 2),
        "instruction_compliance_percent": round(compliance, 2),
        "transparency_score_percent": round(transparency, 2),
        "personality_adherence_percent": round(adherence, 2),
        "proactive_initiative_count": proactive_count,
        "conversation_stability_score": round(stability, 2)
    }


def calculate_defi_metrics(conversation_data: Dict[str, Any]) -> Dict[str, Any]:
    """Calculate DeFi reasoning metrics (optional)."""
    messages = conversation_data.get('messages', [])
    transactions = conversation_data.get('transactions', [])
    
    # DeFi Action Success Rate
    action_types = analyze_action_types(messages, transactions)
    defi_actions = [action for action in action_types if action in ['swap', 'approval', 'mint', 'wrap']]
    defi_success_rate = (len(defi_actions) / len(action_types) * 100) if action_types else 0
    
    # Protocol Selection Accuracy
    protocol_mentions = sum(1 for msg in messages if any(
        protocol in msg.get('content', '').lower()
        for protocol in ['uniswap', 'aave', 'compound', 'dex']
    ))
    protocol_accuracy = min(100, protocol_mentions * 20 + 60)
    
    # Approval Safety Score
    approval_safe = sum(1 for msg in messages if 'approve' in msg.get('content', '').lower() and 'amount' in msg.get('content', '').lower())
    approval_safety = 95.0 if approval_safe > 0 else 100.0
    
    # Sequencing Logic Accuracy
    has_sequence = any('approve' in msg.get('content', '').lower() and 'swap' in messages[i+1].get('content', '').lower() 
                      for i, msg in enumerate(messages[:-1]) if msg.get('role') == 'agent')
    sequencing_accuracy = 90.0 if has_sequence else 70.0
    
    # Slippage Awareness
    slippage_mentions = sum(1 for msg in messages if 'slippage' in msg.get('content', '').lower())
    slippage_awareness = min(100, slippage_mentions * 30 + 50)
    
    return {
        "defi_action_success_rate": round(defi_success_rate, 2),
        "protocol_selection_accuracy": round(protocol_accuracy, 2),
        "approval_safety_score": round(approval_safety, 2),
        "sequencing_logic_accuracy": round(sequencing_accuracy, 2),
        "slippage_awareness": round(slippage_awareness, 2)
    }


def calculate_aggregate_scores(capability: Dict, efficiency: Dict, reliability: Dict, 
                               interaction: Dict, defi: Dict) -> Dict[str, float]:
    """Calculate aggregated evaluation scores."""
    
    # Capability Score (average of main metrics)
    capability_score = (
        capability['action_success_rate'] +
        capability['contract_interaction_accuracy'] +
        capability['state_verification_accuracy']
    ) / 3
    
    # Efficiency Score
    efficiency_score = (
        efficiency['gas_efficiency_percent'] +
        efficiency['transaction_consistency_percent'] +
        (100 - efficiency['failure_rate_percent'])
    ) / 3
    
    # Reliability Score
    reliability_score = (
        reliability['recovery_rate_percent'] +
        reliability['tool_reliability_percent'] +
        reliability['execution_determinism_percent']
    ) / 3
    
    # Interaction Score
    interaction_score = (
        interaction['instruction_compliance_percent'] +
        interaction['transparency_score_percent'] +
        interaction['personality_adherence_percent']
    ) / 3
    
    # DeFi Reasoning Score
    defi_score = (
        defi['defi_action_success_rate'] +
        defi['approval_safety_score'] +
        defi['sequencing_logic_accuracy']
    ) / 3
    
    # Final Performance Index (weighted average)
    final_performance_index = (
        capability_score * 0.25 +
        efficiency_score * 0.20 +
        reliability_score * 0.20 +
        interaction_score * 0.25 +
        defi_score * 0.10
    )
    
    return {
        "capability_score": round(capability_score, 2),
        "efficiency_score": round(efficiency_score, 2),
        "reliability_score": round(reliability_score, 2),
        "interaction_score": round(interaction_score, 2),
        "defi_reasoning_score": round(defi_score, 2),
        "final_performance_index": round(final_performance_index, 2)
    }


def generate_summary_insights(conversation_data: Dict[str, Any], aggregate_scores: Dict) -> Dict[str, str]:
    """Generate qualitative insights and summary."""
    fpi = aggregate_scores['final_performance_index']
    
    # Determine overall assessment level
    if fpi >= 90:
        execution_reliability = "Exceptional"
        transaction_efficiency = "Highly optimized"
    elif fpi >= 75:
        execution_reliability = "High"
        transaction_efficiency = "Above average"
    elif fpi >= 60:
        execution_reliability = "Moderate"
        transaction_efficiency = "Average"
    else:
        execution_reliability = "Needs improvement"
        transaction_efficiency = "Below average"
    
    # Determine response behavior
    interaction_score = aggregate_scores['interaction_score']
    if interaction_score >= 90:
        response_behavior = "Highly responsive and transparent"
    elif interaction_score >= 75:
        response_behavior = "Responsive with good transparency"
    else:
        response_behavior = "Adequate responsiveness"
    
    # Determine DeFi competence
    defi_score = aggregate_scores['defi_reasoning_score']
    if defi_score >= 80:
        defi_competence = "Strong DeFi capabilities demonstrated"
    elif defi_score >= 60:
        defi_competence = "Moderate DeFi capabilities"
    else:
        defi_competence = "Limited by environment constraints"
    
    # Generate assessment
    messages = conversation_data.get('messages', [])
    transactions = conversation_data.get('transactions', [])
    action_types = analyze_action_types(messages, transactions)
    
    general_assessment = f"Performs well for {', '.join(action_types[:3]) if action_types else 'basic operations'}; "
    
    if defi_score < 70:
        general_assessment += "needs better handling of advanced DeFi tasks."
    elif len(transactions) < 2:
        general_assessment += "demonstrates solid fundamentals with room for more complex operations."
    else:
        general_assessment += "shows strong capability across multiple operation types."
    
    return {
        "overall_score": fpi,
        "execution_reliability": execution_reliability,
        "transaction_efficiency": transaction_efficiency,
        "response_behavior": response_behavior,
        "defi_competence": defi_competence,
        "general_assessment": general_assessment
    }


def generate_improvement_areas(capability: Dict, efficiency: Dict, reliability: Dict,
                               interaction: Dict, defi: Dict, conversation_data: Dict[str, Any], 
                               llm: LLM) -> List[Dict[str, str]]:
    """Use LLM to dynamically analyze metrics and conversation data to generate improvement areas."""
    
    # Prepare comprehensive context for LLM analysis
    messages = conversation_data.get('messages', [])
    transactions = conversation_data.get('transactions', [])
    personality_name = conversation_data.get('personality_name', 'Unknown')
    
    # Format conversation for analysis
    conversation_summary = []
    for msg in messages[:10]:  # First 10 messages for context
        role = msg.get('role', 'unknown')
        content = msg.get('content', '')[:200]  # Truncate long messages
        conversation_summary.append(f"{role.upper()}: {content}")
    
    conversation_text = "\n".join(conversation_summary)
    
    # Format transaction data
    transaction_summary = []
    for tx in transactions:
        tx_hash = tx.get('transaction_hash', 'N/A')
        analysis = tx.get('analysis', '')[:150]
        transaction_summary.append(f"TX {tx_hash[:10]}...: {analysis}")
    
    transaction_text = "\n".join(transaction_summary) if transaction_summary else "No transactions recorded"
    
    # Create comprehensive prompt for LLM
    prompt = f"""You are an expert AI agent performance analyst. Analyze the following CDP blockchain agent test results and generate 5-7 specific, actionable improvement areas.

AGENT PERFORMANCE METRICS:

**Capability Metrics:**
- Action Success Rate: {capability['action_success_rate']}%
- Action Types: {', '.join(capability['action_type_coverage'])}
- Contract Interaction Accuracy: {capability['contract_interaction_accuracy']}%
- State Verification: {capability['state_verification_accuracy']}%
- Error Recovery: {capability['adaptive_error_recovery']}%
- Network Handling: {capability['network_handling_score']}%

**Efficiency Metrics:**
- Avg Execution Latency: {efficiency['avg_execution_latency_ms']} ms
- Avg Gas Used: {efficiency['avg_gas_used']} units
- Gas Efficiency: {efficiency['gas_efficiency_percent']}%
- Failure Rate: {efficiency['failure_rate_percent']}%

**Reliability Metrics:**
- Recovery Rate: {reliability['recovery_rate_percent']}%
- Tool Reliability: {reliability['tool_reliability_percent']}%
- Execution Determinism: {reliability['execution_determinism_percent']}%
- Network Adaptability: {reliability['network_adaptability_score']}%

**Interaction Metrics:**
- Response Latency: {interaction['response_latency_ms']} ms
- Instruction Compliance: {interaction['instruction_compliance_percent']}%
- Transparency: {interaction['transparency_score_percent']}%
- Personality Adherence: {interaction['personality_adherence_percent']}%
- Proactive Initiatives: {interaction['proactive_initiative_count']}

**DeFi Reasoning Metrics:**
- DeFi Action Success: {defi['defi_action_success_rate']}%
- Protocol Selection: {defi['protocol_selection_accuracy']}%
- Approval Safety: {defi['approval_safety_score']}%
- Sequencing Logic: {defi['sequencing_logic_accuracy']}%
- Slippage Awareness: {defi['slippage_awareness']}%

**Personality Tested:** {personality_name}

**Conversation Sample:**
{conversation_text}

**Transaction Analysis:**
{transaction_text}

**TASK:** Based on this comprehensive data, generate 5-7 specific improvement areas. Consider:
1. Actual weaknesses shown in the metrics (scores < 90%)
2. Missing capabilities (action types not demonstrated)
3. Inefficiencies in the conversation flow
4. DeFi reasoning gaps
5. Performance bottlenecks
6. Areas where the agent could be more proactive
7. Edge cases not handled well

For each improvement area, provide:
- area: Short title (3-6 words)
- scope: What's currently happening or missing (1 sentence)
- suggestion: Specific, actionable improvement (1-2 sentences)
- priority: One of [CRITICAL, HIGH, MEDIUM, LOW]

Priority Guidelines:
- CRITICAL: Blocks core functionality (success rate < 70%, critical failures)
- HIGH: Significantly impacts user experience (scores 70-85%)
- MEDIUM: Room for improvement (scores 85-95%)
- LOW: Optimization opportunities (scores > 95%)

Return ONLY valid JSON in this exact format:
[
  {{
    "area": "Area Title",
    "scope": "Current situation or gap",
    "suggestion": "Specific actionable improvement",
    "priority": "HIGH"
  }}
]

CRITICAL: Return ONLY the JSON array. No markdown, no explanations, no additional text."""

    try:
        print(f"🤖 Generating AI-powered improvement areas...")
        response = llm.create_completion(prompt, temperature=0.4)
        
        # Clean the response
        cleaned_response = clean_json_response(response)
        
        # Parse JSON
        improvements = json.loads(cleaned_response)
        
        if not isinstance(improvements, list):
            raise ValueError("Response is not a list")
        
        # Validate structure
        valid_improvements = []
        for imp in improvements:
            if all(key in imp for key in ['area', 'scope', 'suggestion', 'priority']):
                valid_improvements.append(imp)
        
        if not valid_improvements:
            raise ValueError("No valid improvements found")
        
        # Sort by priority
        priority_order = {"CRITICAL": 0, "HIGH": 1, "MEDIUM": 2, "LOW": 3}
        valid_improvements.sort(key=lambda x: priority_order.get(x.get('priority', 'LOW'), 3))
        
        print(f"✅ Generated {len(valid_improvements)} AI-powered improvement areas")
        return valid_improvements
        
    except Exception as e:
        print(f"⚠️  Error generating AI improvements: {e}")
        print(f"   Falling back to basic analysis")
        
        # Fallback: Generate basic improvements based on lowest scores
        fallback_improvements = []
        
        # Find weakest areas
        all_scores = {
            "DeFi Coverage": defi['defi_action_success_rate'],
            "Error Recovery": reliability['recovery_rate_percent'],
            "Gas Efficiency": efficiency['gas_efficiency_percent'],
            "Action Success Rate": capability['action_success_rate'],
            "Protocol Knowledge": (defi['protocol_selection_accuracy'] + defi['slippage_awareness']) / 2,
            "Response Time": 100 - min(100, (interaction['response_latency_ms'] / 50)),
        }
        
        # Sort by score (lowest first)
        sorted_areas = sorted(all_scores.items(), key=lambda x: x[1])
        
        for area, score in sorted_areas[:5]:
            if score < 90:
                priority = "HIGH" if score < 75 else "MEDIUM"
            else:
                priority = "LOW"
                
            fallback_improvements.append({
                "area": f"Improve {area}",
                "scope": f"Current score: {score:.1f}% - Room for optimization",
                "suggestion": f"Analyze conversation patterns and optimize {area.lower()} through targeted improvements",
                "priority": priority
            })
        
        return fallback_improvements


def generate_comprehensive_metrics(conversation_data: Dict[str, Any]) -> Dict[str, Any]:
    """Generate comprehensive metrics for a conversation or test run."""
    
    print(f"\n{'='*80}")
    print(f"🔍 GENERATING COMPREHENSIVE METRICS")
    print(f"{'='*80}\n")
    
    # Check if this is a comprehensive test run or single conversation
    if 'conversations' in conversation_data and 'all_transactions' in conversation_data:
        # This is a comprehensive test run - process all conversations
        print(f"📊 Processing comprehensive test run with {conversation_data.get('total_conversations', 0)} conversations")
        print(f"📊 Personalities: {conversation_data.get('personalities', [])}")
        print(f"📊 Total transactions: {conversation_data.get('total_transactions', 0)}")
        
        # Combine all conversations and transactions for comprehensive analysis
        all_messages = []
        all_transactions = conversation_data.get('all_transactions', [])
        
        for conv in conversation_data.get('conversations', []):
            all_messages.extend(conv.get('messages', []))
            # Also add individual conversation transactions
            all_transactions.extend(conv.get('transactions', []))
        
        # Create combined data structure for metrics calculation
        combined_data = {
            'messages': all_messages,
            'transactions': all_transactions,
            'conversation_id': conversation_data.get('conversation_id'),
            'personality_name': f"Multi-Personality Test Run ({len(conversation_data.get('personalities', []))} personalities)",
            'test_run_id': conversation_data.get('test_run_id'),
            'test_run_timestamp': conversation_data.get('test_run_timestamp'),
            'personalities': conversation_data.get('personalities', []),
            'total_conversations': conversation_data.get('total_conversations', 0),
            'total_transactions': conversation_data.get('total_transactions', 0)
        }
        
        print(f"📊 Combined analysis: {len(all_messages)} messages, {len(all_transactions)} transactions")
        
    else:
        # This is a single conversation - use existing logic
        print(f"📊 Processing single conversation: {conversation_data.get('conversation_id')}")
        combined_data = conversation_data
    
    # Calculate all metric categories using combined data
    print("📊 Calculating capability metrics...")
    capability_metrics = calculate_capability_metrics(combined_data)
    
    print("⚡ Calculating efficiency metrics...")
    efficiency_metrics = calculate_efficiency_metrics(combined_data)
    
    print("🛡️  Calculating reliability metrics...")
    reliability_metrics = calculate_reliability_metrics(combined_data)
    
    print("💬 Calculating interaction metrics...")
    interaction_metrics = calculate_interaction_metrics(combined_data)
    
    print("🏦 Calculating DeFi reasoning metrics...")
    defi_metrics = calculate_defi_metrics(combined_data)
    
    print("🎯 Calculating aggregate scores...")
    aggregate_scores = calculate_aggregate_scores(
        capability_metrics,
        efficiency_metrics,
        reliability_metrics,
        interaction_metrics,
        defi_metrics
    )
    
    print("📝 Generating summary insights...")
    summary = generate_summary_insights(combined_data, aggregate_scores)
    
    print("🔧 Identifying improvement areas with AI analysis...")
    improvements = generate_improvement_areas(
        capability_metrics,
        efficiency_metrics,
        reliability_metrics,
        interaction_metrics,
        defi_metrics,
        combined_data,
        llm
    )
    
    # Create metrics structure
    metrics = {
        "test_id": combined_data.get('conversation_id'),
        "personality_name": combined_data.get('personality_name'),
        "network": "Base Sepolia",
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "test_run_id": combined_data.get('test_run_id'),
        "test_run_timestamp": combined_data.get('test_run_timestamp'),
        "personalities": combined_data.get('personalities', []),
        "total_conversations": combined_data.get('total_conversations', 1),
        "total_transactions": combined_data.get('total_transactions', 0),
        "metrics": {
            "capability": capability_metrics,
            "efficiency": efficiency_metrics,
            "reliability": reliability_metrics,
            "interaction": interaction_metrics,
            "defi_reasoning": defi_metrics,
            "aggregate_scores": aggregate_scores
        },
        "summary": summary,
        "improvement_areas": improvements
    }
    
    print(f"\n✅ Metrics generation complete!")
    print(f"📊 Final Performance Index: {aggregate_scores['final_performance_index']}/100")
    if 'personalities' in combined_data:
        print(f"📊 Processed {len(combined_data['personalities'])} personalities")
    print()
    
    return metrics


def display_metrics_in_terminal(metrics: Dict[str, Any]):
    """Display metrics in a beautiful terminal format."""
    
    print(f"\n{'='*80}")
    print(f"🎯 FIREGLOBE AGENT PERFORMANCE METRICS")
    print(f"{'='*80}\n")
    
    # Header
    print(f"🆔 Test ID: {metrics['test_id']}")
    print(f"👤 Personality: {metrics['personality_name']}")
    print(f"🌐 Network: {metrics['network']}")
    print(f"⏰ Generated: {metrics['timestamp']}")
    
    # Show comprehensive test run info if available
    if 'test_run_id' in metrics and metrics['test_run_id']:
        print(f"🏃 Test Run ID: {metrics['test_run_id']}")
        print(f"📊 Total Conversations: {metrics.get('total_conversations', 1)}")
        print(f"📊 Total Transactions: {metrics.get('total_transactions', 0)}")
        if 'personalities' in metrics and metrics['personalities']:
            print(f"👥 Personalities: {', '.join(metrics['personalities'])}")
    print()
    
    print(f"{'='*80}")
    print(f"📊 AGGREGATE SCORES")
    print(f"{'='*80}\n")
    
    agg = metrics['metrics']['aggregate_scores']
    print(f"  🎯 Final Performance Index (FPI): {agg['final_performance_index']}/100")
    print(f"  💪 Capability Score:               {agg['capability_score']}/100")
    print(f"  ⚡ Efficiency Score:               {agg['efficiency_score']}/100")
    print(f"  🛡️  Reliability Score:              {agg['reliability_score']}/100")
    print(f"  💬 Interaction Score:              {agg['interaction_score']}/100")
    print(f"  🏦 DeFi Reasoning Score:           {agg['defi_reasoning_score']}/100\n")
    
    print(f"{'='*80}")
    print(f"💪 CAPABILITY METRICS")
    print(f"{'='*80}\n")
    
    cap = metrics['metrics']['capability']
    print(f"  ✅ Action Success Rate:             {cap['action_success_rate']}%")
    print(f"  🔧 Action Types Covered:            {', '.join(cap['action_type_coverage']) if cap['action_type_coverage'] else 'None'}")
    print(f"  📝 Contract Interaction Accuracy:   {cap['contract_interaction_accuracy']}%")
    print(f"  🔍 State Verification Accuracy:     {cap['state_verification_accuracy']}%")
    print(f"  🔄 Adaptive Error Recovery:         {cap['adaptive_error_recovery']}%")
    print(f"  🌐 Network Handling Score:          {cap['network_handling_score']}%\n")
    
    print(f"{'='*80}")
    print(f"⚡ EFFICIENCY METRICS")
    print(f"{'='*80}\n")
    
    eff = metrics['metrics']['efficiency']
    print(f"  ⏱️  Avg Execution Latency:           {eff['avg_execution_latency_ms']} ms")
    print(f"  ⛽ Avg Gas Used:                    {eff['avg_gas_used']} units")
    print(f"  📊 Gas Efficiency:                  {eff['gas_efficiency_percent']}%")
    print(f"  💰 Cost per Action:                 {eff['cost_per_successful_action_eth']} ETH")
    print(f"  📈 Transaction Consistency:         {eff['transaction_consistency_percent']}%")
    print(f"  ❌ Failure Rate:                    {eff['failure_rate_percent']}%\n")
    
    print(f"{'='*80}")
    print(f"🛡️  RELIABILITY METRICS")
    print(f"{'='*80}\n")
    
    rel = metrics['metrics']['reliability']
    print(f"  🔄 Recovery Rate:                   {rel['recovery_rate_percent']}%")
    print(f"  🔧 Tool Reliability:                {rel['tool_reliability_percent']}%")
    print(f"  🎯 Execution Determinism:           {rel['execution_determinism_percent']}%")
    print(f"  🌐 Network Adaptability:            {rel['network_adaptability_score']}%\n")
    
    print(f"{'='*80}")
    print(f"💬 INTERACTION METRICS")
    print(f"{'='*80}\n")
    
    inter = metrics['metrics']['interaction']
    print(f"  ⏱️  Response Latency:                {inter['response_latency_ms']} ms")
    print(f"  ✅ Instruction Compliance:          {inter['instruction_compliance_percent']}%")
    print(f"  📊 Transparency Score:              {inter['transparency_score_percent']}%")
    print(f"  👤 Personality Adherence:           {inter['personality_adherence_percent']}%")
    print(f"  💡 Proactive Initiative Count:      {inter['proactive_initiative_count']}")
    print(f"  🎯 Conversation Stability:          {inter['conversation_stability_score']}%\n")
    
    print(f"{'='*80}")
    print(f"🏦 DEFI REASONING METRICS")
    print(f"{'='*80}\n")
    
    defi = metrics['metrics']['defi_reasoning']
    print(f"  ✅ DeFi Action Success Rate:        {defi['defi_action_success_rate']}%")
    print(f"  🎯 Protocol Selection Accuracy:     {defi['protocol_selection_accuracy']}%")
    print(f"  🔒 Approval Safety Score:           {defi['approval_safety_score']}%")
    print(f"  🔄 Sequencing Logic Accuracy:       {defi['sequencing_logic_accuracy']}%")
    print(f"  📊 Slippage Awareness:              {defi['slippage_awareness']}%\n")
    
    print(f"{'='*80}")
    print(f"📝 SUMMARY & INSIGHTS")
    print(f"{'='*80}\n")
    
    summary = metrics['summary']
    print(f"  🎯 Overall Score:        {summary['overall_score']}/100")
    print(f"  🛡️  Execution Reliability: {summary['execution_reliability']}")
    print(f"  ⚡ Transaction Efficiency: {summary['transaction_efficiency']}")
    print(f"  💬 Response Behavior:     {summary['response_behavior']}")
    print(f"  🏦 DeFi Competence:       {summary['defi_competence']}")
    print(f"\n  📋 Assessment: {summary['general_assessment']}\n")
    
    if metrics['improvement_areas']:
        print(f"{'='*80}")
        print(f"🔧 IMPROVEMENT AREAS (Sorted by Priority)")
        print(f"{'='*80}\n")
        
        priority_icons = {
            "CRITICAL": "🚨",
            "HIGH": "⚠️",
            "MEDIUM": "📌",
            "LOW": "💡"
        }
        
        for i, area in enumerate(metrics['improvement_areas'], 1):
            priority = area.get('priority', 'LOW')
            icon = priority_icons.get(priority, '📋')
            print(f"  {i}. {icon} {area['area']} [{priority}]")
            print(f"     Scope: {area['scope']}")
            print(f"     Suggestion: {area['suggestion']}\n")
    
    print(f"{'='*80}\n")


# Initialize global components
metta = MeTTa()
initialize_knowledge_graph(metta)
rag = MetricsRAG(metta, BACKEND_URL)
llm = LLM(api_key=ASI_ONE_API_KEY)

# Global storage for last metrics
last_metrics_data = None
last_conversation_id = None


# Startup Handler
@agent.on_event("startup")
async def startup_handler(ctx: Context):
    ctx.logger.info(f"FireGlobe Agent Metrics Generator started with address: {ctx.agent.address}")
    ctx.logger.info("🎯 Ready to generate comprehensive agent performance metrics!")
    ctx.logger.info(f"📡 Connected to backend: {BACKEND_URL}")
    ctx.logger.info("🧠 Powered by ASI:One AI and MeTTa Knowledge Graph")
    ctx.logger.info("\n🌐 REST API Endpoints:")
    ctx.logger.info("  - POST http://localhost:8081/metrics/generate")
    ctx.logger.info("  - GET  http://localhost:8081/metrics/last")


# REST API Handlers
@agent.on_rest_post("/metrics/generate", MetricsGenerationRequest, MetricsGenerationResponse)
async def handle_metrics_generation(ctx: Context, req: MetricsGenerationRequest) -> MetricsGenerationResponse:
    """Generate comprehensive metrics for a conversation."""
    ctx.logger.info("📊 Received metrics generation request")
    
    try:
        # Get conversation data
        if req.conversation_id:
            ctx.logger.info(f"🔍 Fetching conversation by ID: {req.conversation_id}")
            conversation_data = rag.get_conversation_by_id(req.conversation_id)
        else:
            ctx.logger.info("🔍 Fetching last conversation from Knowledge Graph")
            conversation_data = rag.get_last_conversation()
        
        if not conversation_data:
            return MetricsGenerationResponse(
                success=False,
                conversation_id="unknown",
                metrics={},
                timestamp=datetime.now(timezone.utc).isoformat(),
                agent_address=str(ctx.agent.address)
            )
        
        conversation_id = conversation_data.get('conversation_id', 'unknown')
        ctx.logger.info(f"✅ Retrieved conversation: {conversation_id}")
        
        # Generate comprehensive metrics
        metrics = generate_comprehensive_metrics(conversation_data)
        
        # Store metrics globally
        global last_metrics_data, last_conversation_id
        last_metrics_data = metrics
        last_conversation_id = conversation_id
        
        # Store in local KG
        rag.store_metrics(conversation_id, metrics)
        
        # Display metrics in terminal
        display_metrics_in_terminal(metrics)
        
        return MetricsGenerationResponse(
            success=True,
            conversation_id=conversation_id,
            metrics=metrics,
            timestamp=datetime.now(timezone.utc).isoformat(),
            agent_address=str(ctx.agent.address)
        )
        
    except Exception as e:
        ctx.logger.error(f"❌ Error generating metrics: {e}")
        import traceback
        ctx.logger.error(f"Traceback: {traceback.format_exc()}")
        
        return MetricsGenerationResponse(
            success=False,
            conversation_id="unknown",
            metrics={},
            timestamp=datetime.now(timezone.utc).isoformat(),
            agent_address=str(ctx.agent.address)
        )


@agent.on_rest_get("/metrics/last", LastMetricsResponse)
async def handle_last_metrics(ctx: Context) -> LastMetricsResponse:
    """Get the last generated metrics."""
    ctx.logger.info("📊 Received request for last metrics")
    
    try:
        global last_metrics_data, last_conversation_id
        
        if last_metrics_data and last_conversation_id:
            return LastMetricsResponse(
                success=True,
                conversation_id=last_conversation_id,
                metrics=last_metrics_data,
                timestamp=datetime.now(timezone.utc).isoformat(),
                agent_address=str(ctx.agent.address)
            )
        else:
            return LastMetricsResponse(
                success=False,
                conversation_id=None,
                metrics=None,
                timestamp=datetime.now(timezone.utc).isoformat(),
                agent_address=str(ctx.agent.address)
            )
        
    except Exception as e:
        ctx.logger.error(f"❌ Error retrieving last metrics: {e}")
        
        return LastMetricsResponse(
            success=False,
            conversation_id=None,
            metrics=None,
            timestamp=datetime.now(timezone.utc).isoformat(),
            agent_address=str(ctx.agent.address)
        )


if __name__ == '__main__':
    print("🚀 Starting FireGlobe Agent Metrics Generator...")
    print(f"✅ Agent address: {agent.address}")
    print(f"📡 Backend URL: {BACKEND_URL}")
    print("🧠 Powered by ASI:One AI and MeTTa Knowledge Graph")
    print("\n🌐 REST API Endpoints:")
    print("  POST http://localhost:8081/metrics/generate")
    print("  Body: {\"conversation_id\": \"<optional-conversation-id>\"}")
    print("  Returns: Comprehensive agent performance metrics")
    print("\n  GET http://localhost:8081/metrics/last")
    print("  Returns: Last generated metrics\n")
    print("📊 Metrics Include:")
    print("  - Capability Metrics (action success, coverage, accuracy)")
    print("  - Efficiency Metrics (latency, gas, cost)")
    print("  - Reliability Metrics (recovery, tool reliability)")
    print("  - Interaction Metrics (compliance, transparency)")
    print("  - DeFi Reasoning Metrics (protocol selection, safety)")
    print("  - Aggregate Scores & Final Performance Index")
    print("  - Summary Insights & Improvement Areas")
    print("\nPress CTRL+C to stop the agent\n")
    
    try:
        agent.run()
    except KeyboardInterrupt:
        print("\n🛑 Shutting down CDP Agent Metrics Generator...")
        print("✅ Agent stopped.")

